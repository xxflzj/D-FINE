
## 🔍 探索D-FINE背后的关键创新

### D-FINE重新定义了基于DETR的目标检测器中的回归任务。

### FDR：将检测框的生成过程拆解为两个步骤：

1. **初始框预测**：与传统方法类似，首先在解码器的第一层生成初始边界框。
2. **精细分布优化**：模型解码层迭代地对四组概率分布进行逐层迭代优化。这些分布作为检测框的一种细粒度中间表征，能够对初始边界框的上下左右边缘进行细微的小幅度修正亦或是大幅度的搬移。

### FDR的主要优势：
1. **简化的监督**：在依赖传统的L1损失、IOU损失优化检测框的同时，可以用标签和预测结果之间的“残差”约束中间态的概率分布函数。这使每个解码层能够更有效地关注并解决其当前面临的定位误差，随着层数加深，其优化目标变得越来越简单，从而简化了整体优化过程。

2. **复杂场景下的鲁棒性**：这些概率分布的值本质上代表了对每个边界“微调”的自信程度。这使检测器能够在不同网络深度独立建模每个边界的不确定性，从而在遮挡、运动模糊和低光照等复杂的实际场景下表现出更强的鲁棒性，相比直接回归四个固定值要更为稳健。

4. **灵活的优化机制**：概率分布通过加权求和转化为最终的边界框偏移值。精心设计的加权函数确保在初始框准确时进行细微调整，而在必要时则提供较大的修正。

6. **研究潜力与可扩展性**：FDR通过将回归任务转变为同分类任务一致的概率分布预测问题，不仅提高了与其他任务的兼容性，还使得目标检测模型可以受益于知识蒸馏、多任务学习和分布建模等更多领域的创新，为未来的研究打开了新的大门。



<p align="center">
    <img src="https://raw.githubusercontent.com/Peterande/storage/master/figs/fdr-1.jpg" alt="精细分布优化过程" width="666">
</p>

### GO-LSD：将知识蒸馏应用到FDR框架检测器的探索

搭载FDR的检测器满足以下两点：

1. **知识传递**：网络输出变成了概率分布，而概率分布携带定位知识，可以通过计算KLD损失从深层传递到浅层。这是传统固定框表示（狄拉克δ函数）无法实现的。
   
3. **一致的优化目标**：由于每一层都共享一个共同目标：减少初始边界框与真实边界框之间的残差；因此最后一层生成的精确概率分布可以通过蒸馏引导前几层。这产生了一种双赢的协同效应：随着训练的进行，最后一层的预测变得越来越准确，其生成的软标签更好地帮助前几层提高预测准确性。反过来，前几层学会更快地定位到准确位置，简化了深层的优化任务，进一步提高了整体准确性。

于是，基于FDR，我们提出了GO-LSD（全局最优定位自蒸馏）。通过在网络层间实现定位知识蒸馏，进一步扩展了D-FINE的能力。

<p align="center">
    <img src="https://raw.githubusercontent.com/Peterande/storage/master/figs/go_lsd-1.jpg" alt="GO-LSD过程" width="666">
</p>

### 问题1：FDR和GO-LSD会带来更多的推理成本吗？
并不会，FDR和原始的预测几乎没有在速度、参数量和计算复杂度上的任何区别，完全是无感替换。

### 问题2：FDR和GO-LSD会带来更多的训练成本吗？
训练成本的增加主要来源于如何生成分布的标签。我们已经对该过程进行了优化，将训练时长和显存占用控制在了6%和2%，几乎无感。

### D-FINE预测的可视化

以下可视化展示了D-FINE在各种复杂检测场景中的预测结果。这些场景包括遮挡、低光照、运动模糊、景深效果和密集场景。尽管面对这些挑战，D-FINE依然能够产生准确的定位结果。


<p align="center">
    <img src="https://raw.githubusercontent.com/Peterande/storage/master/figs/hard_case-1.jpg" alt="D-FINE在复杂场景中的预测" width="666">
</p>

